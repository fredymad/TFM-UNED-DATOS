{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Sr4O8GffRq"
      },
      "source": [
        "Codigo para transformar el corpus de entrada anotado en formato BRAT Standoff en formato IOB.\n",
        "\n",
        "\n",
        "Para ello usamos el tokenizador de Spacy. Hay dos versiones posibles en español:\n",
        "\n",
        "\n",
        "1.   !python -m spacy download es_core_news_md\n",
        "2.   !python -m spacy download es_core_news_sm\n",
        "\n",
        "\n",
        "Y por cada versión sm, md o lg, en función del tamaño del modelo. Esto se puede ver [en el siguiente enlace](https://stackoverflow.com/questions/50487495/what-is-difference-between-en-core-web-sm-en-core-web-mdand-en-core-web-lg-mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wh7K_WIQUsF"
      },
      "source": [
        "Establecemos conexión con Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4obuXPRKM6G",
        "outputId": "981bf2db-22fb-49e5-cd33-45853ef4df8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#Enlazamos nuestro notebook en Colab con nuestro almacenamiento en Google Drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkSObnq6QbVi"
      },
      "source": [
        "Instalamos spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A4-YQMUI8HW",
        "outputId": "0e0c0af2-9b9d-405e-85c1-ae21376b3a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-19 22:47:25.125048: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.4.0/es_core_news_md-3.4.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.3 MB 194 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-md==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-19 22:47:41.326883: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 216 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# install the requirements\n",
        "!pip install spacy\n",
        "\n",
        "# Instalamos los modelos\n",
        "#!python -m spacy download en_core_web_md\n",
        "#!python -m spacy download en_core_news_md\n",
        "#!python -m spacy download es_core_web_md\n",
        "!python -m spacy download es_core_news_sm # El que recomienda spacy por defecto\n",
        "#!python -m spacy download es_core_news_md # El primero usado por mi\n",
        "#!python -m spacy download es_core_news_lg #\n",
        "\n",
        "\n",
        "# Concatenar ann data\n",
        "# No nos sirve, finalmente no hacemos uso de esta librería\n",
        "#!pip install anndata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTczmofWQj0h"
      },
      "source": [
        "# Código de Oswaldo `parserBIO_v3_may8_2021.py`\n",
        "Adaptamos el código que nos dejo Oswaldo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe40NRZwIok7"
      },
      "outputs": [],
      "source": [
        "# Cargamos las librerías necesarias\n",
        "import spacy\n",
        "import os\n",
        "import re\n",
        "#import en_core_web_sm\n",
        "import es_core_news_md #Importante que sea el espanol pues los textos están escritos en castellano\n",
        "import es_core_news_sm\n",
        "import es_core_news_lg\n",
        "\n",
        "#nlp = en_core_web_sm.load()\n",
        "\n",
        "nlp = es_core_news_md.load()\n",
        "\n",
        "BEGIN = 'B'\n",
        "INSIDE = 'I'\n",
        "OUTSIDE = 'O'\n",
        "\n",
        "def getDictWords(text):\n",
        "\twords = {}\n",
        "\tent = nlp(text)\n",
        "\tfor i in range(len(ent)):\n",
        "\t\twords[ent[i].idx] = ent[i].text\n",
        "    \t\n",
        "\treturn words\n",
        "\n",
        "def getDictEntities(file_ann):\n",
        "  entities = {}\n",
        "  with open(file_ann) as anns:\n",
        "    for ann in anns:\n",
        "\n",
        "      start = int(ann[:-1].split('\\t')[1].split(' ')[1])\n",
        "      end = int(ann[:-1].split('\\t')[1].split(' ')[2])\n",
        "\n",
        "      entities[(start, end)] = [ann[:-1].split('\\t')[2],ann.split('\\t')[1].split(' ')[0]]\n",
        "\n",
        "  return entities\n",
        "\n",
        "def is_empty(string): \n",
        "\treturn not string.strip() \n",
        "\n",
        "\n",
        "def getBIO(words,entities):\n",
        "\tbio = []\n",
        "\tkeys = words.keys()\n",
        "\tfound = False\n",
        "\tprocessedConcepts = []\n",
        "\n",
        "\tfor key in keys:\n",
        "\t\tif(not words[key] in processedConcepts):\n",
        "\t\t\tkeysEntities = entities.keys()\n",
        "\t\t\tfor keyEntity in keysEntities:\n",
        "\t\t\t\tstart = keyEntity[0]\n",
        "\t\t\t\tend = keyEntity[1]\n",
        "\t\t\t\tif(key==start):\n",
        "\t\t\t\t\tconcept = entities[keyEntity]\n",
        "\t\t\t\t\tif(len(concept[0])==1):\n",
        "\t\t\t\t\t\tbio.append(concept[0]+\"\\t\"+BEGIN+\"_\"+concept[1]+\"\\n\")\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tent = nlp(concept[0])\n",
        "\t\t\t\t\t\tbio.append(ent[0].text+\"\\t\"+BEGIN+\"_\"+concept[1]+\"\\n\")\n",
        "\n",
        "\t\t\t\t\t\tfor i in range(len(ent)-1):\n",
        "\t\t\t\t\t\t\tspaces = (ent[i + 1].idx) - (ent[i].idx + len(ent[i].text))\n",
        "\t\t\t\t\t\t\tstart = start + len(ent[i].text) + spaces\n",
        "\t\t\t\t\t\t\tbio.append(ent[i+1].text+\"\\t\"+INSIDE+\"_\"+concept[1]+\"\\n\")\n",
        "\t\t\t\t\t\t\tprocessedConcepts.append(ent[i+1].text)\n",
        "\t\t\t\t\t\tfound = True\n",
        "\n",
        "\t\t\tif (not found):\n",
        "\t\t\t\tif(not(is_empty(words[key]))):\n",
        "\t\t\t\t\tbio.append(words[key]+\"\\t\"+OUTSIDE+\"\\n\")\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tif(\".\" == words[key]):\n",
        "\t\t\t\t\t\tbio.append(\"\\n\")\n",
        "\t\t\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tbio.append(\"\\n\")\n",
        "\t\t\n",
        "\t\t\tfound = False\n",
        "\t\telse:\n",
        "\t\t\tprocessedConcepts.remove(words[key])\n",
        "\t\t\n",
        "\treturn bio\n",
        "\n",
        "def bioToFile(pathOutput,bio):\n",
        "\tf = open(pathOutput,\"w\")\n",
        "\tfor i in range(len(bio)):\n",
        "\t\tf.write(bio[i])\n",
        "\tf.close()\n",
        "\n",
        "\n",
        "def readFile(pathFile):\n",
        "\tf = open(pathFile,'r') \n",
        "\ttext = f.read()\n",
        "\tf.close()\n",
        "\treturn text\t\n",
        "\n",
        "# Lo hacemos con md\n",
        "\n",
        "# Se eligen las carpetas que contienen tanto los archivos en .txt como las anotaciones .ann \n",
        "\n",
        "# Conjunto de entrenamiento\n",
        "# TASK 1\n",
        "#path = '/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task1'\n",
        "#path_Output = '/content/drive/MyDrive/Colab Notebooks/Corpus/Procesado/Oswaldo/Train/task1converted/'\n",
        "\n",
        "# TASK 2\n",
        "#path = '/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task2'\n",
        "#path_Output = '/content/drive/MyDrive/Colab Notebooks/Corpus/Procesado/Oswaldo/Train/task2converted/'\n",
        "\n",
        "# Conjunto de test\n",
        "# TASK 1\n",
        "#path = '/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/ner'\n",
        "#path_Output = '/content/drive/MyDrive/Colab Notebooks/Corpus/Procesado/Oswaldo/Test/nerconverted/'\n",
        "\n",
        "# TASK2\n",
        "#path = '/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/class'\n",
        "#path_Output = '/content/drive/MyDrive/Colab Notebooks/Corpus/Procesado/Oswaldo/Test/classconverted/'\n",
        "\n",
        "files = []\n",
        "# r=root, d=directories, f = files\n",
        "for r, d, f in os.walk(path):\n",
        "\tfor file in f:\n",
        "\t\tif file.endswith(\".txt\"):\n",
        "\t\t\tfiles.append(os.path.join(r, file))\n",
        "\n",
        "\tfor file in files:\n",
        "\t\ttext = readFile(file)\n",
        "\t\twords = getDictWords(text)\n",
        "\t\tentities = getDictEntities(file.replace(\".txt\",\".ann\"))\n",
        "\t\tbio = getBIO(words,entities)\n",
        "\t\thead, tail = os.path.split(file)\n",
        "\t\tbioToFile(path_Output+tail.replace(\".txt\",\".bio\"),bio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm31c8x4VvYI"
      },
      "source": [
        "# Código Neuroner \n",
        "Finalmente optamos por usar el código de NeuroNER\n",
        "\n",
        "https://github.com/Franck-Dernoncourt/NeuroNER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NVqpuBVCvCp",
        "outputId": "e389d64d-0723-4823-8261-fe976e62b057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycorenlp\n",
            "  Downloading pycorenlp-0.3.0.tar.gz (1.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pycorenlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp) (3.0.4)\n",
            "Building wheels for collected packages: pycorenlp\n",
            "  Building wheel for pycorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-py3-none-any.whl size=2144 sha256=d7c8392fe1b3913ccc54fc224e91610d69d4650630aac41955efa17ea8532fb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/65/32/176699d5db77f83d0510bd5ce6a5a41397125ce84f9c75e329\n",
            "Successfully built pycorenlp\n",
            "Installing collected packages: pycorenlp\n",
            "Successfully installed pycorenlp-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyneuroner[cpu]\n",
            "  Downloading pyneuroner-1.0.8-py2.py3-none-any.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 17.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.8/dist-packages (from pyneuroner[cpu]) (3.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.8/dist-packages (from pyneuroner[cpu]) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyneuroner[cpu]) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from pyneuroner[cpu]) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from pyneuroner[cpu]) (2.8.8)\n",
            "Requirement already satisfied: pycorenlp>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pyneuroner[cpu]) (0.3.0)\n",
            "Requirement already satisfied: tensorflow>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from pyneuroner[cpu]) (2.9.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.2->pyneuroner[cpu]) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pycorenlp>=0.3.0->pyneuroner[cpu]) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.2->pyneuroner[cpu]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.2->pyneuroner[cpu]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.2->pyneuroner[cpu]) (3.1.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (4.64.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (8.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (1.10.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->pyneuroner[cpu]) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2.0.18->pyneuroner[cpu]) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pycorenlp>=0.3.0->pyneuroner[cpu]) (2022.12.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (2.1.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.3.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (2.9.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (0.28.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.51.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (3.19.6)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (2.9.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (1.12)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=1.12.0->pyneuroner[cpu]) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=1.12.0->pyneuroner[cpu]) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (2.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=1.12.0->pyneuroner[cpu]) (3.2.2)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->pyneuroner[cpu]) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->pyneuroner[cpu]) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=2.0.18->pyneuroner[cpu]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy>=2.0.18->pyneuroner[cpu]) (2.0.1)\n",
            "Installing collected packages: pyneuroner\n",
            "Successfully installed pyneuroner-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pycorenlp\n",
        "!pip3 install pyneuroner[cpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YC2Np5QCY29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcbd270b-cc52-447a-fb8c-7dbfc5797ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import codecs\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "import spacy\n",
        "\n",
        "from neuroner import utils_nlp\n",
        "\n",
        "def get_start_and_end_offset_of_token_from_spacy(token):\n",
        "    start = token.idx\n",
        "    end = start + len(token)\n",
        "    return start, end\n",
        "\n",
        "def get_sentences_and_tokens_from_spacy(text, spacy_nlp):\n",
        "    document = spacy_nlp(text)\n",
        "    # sentences\n",
        "    sentences = []\n",
        "    for span in document.sents:\n",
        "        sentence = [document[i] for i in range(span.start, span.end)]\n",
        "        sentence_tokens = []\n",
        "        for token in sentence:\n",
        "            token_dict = {}\n",
        "            token_dict['start'], token_dict['end'] = get_start_and_end_offset_of_token_from_spacy(token)\n",
        "            token_dict['text'] = text[token_dict['start']:token_dict['end']]\n",
        "            if token_dict['text'].strip() in ['\\n', '\\t', ' ', '']:\n",
        "                continue\n",
        "            # Make sure that the token text does not contain any space\n",
        "            if len(token_dict['text'].split(' ')) != 1:\n",
        "                print(\"WARNING: the text of the token contains space character, replaced with hyphen\\n\\t{0}\\n\\t{1}\".format(token_dict['text'], \n",
        "                                                                                                                           token_dict['text'].replace(' ', '-')))\n",
        "                token_dict['text'] = token_dict['text'].replace(' ', '-')\n",
        "            sentence_tokens.append(token_dict)\n",
        "        sentences.append(sentence_tokens)\n",
        "    return sentences\n",
        "\n",
        "def get_stanford_annotations(text, core_nlp, port=9000, annotators='tokenize,ssplit,pos,lemma'):\n",
        "    output = core_nlp.annotate(text, properties={\n",
        "        \"timeout\": \"10000\",\n",
        "        \"ssplit.newlineIsSentenceBreak\": \"two\",\n",
        "        'annotators': annotators,\n",
        "        'outputFormat': 'json'\n",
        "    })\n",
        "    if type(output) is str:\n",
        "        output = json.loads(output, strict=False)\n",
        "    return output\n",
        "\n",
        "def get_sentences_and_tokens_from_stanford(text, core_nlp):\n",
        "    stanford_output = get_stanford_annotations(text, core_nlp)\n",
        "    sentences = []\n",
        "    for sentence in stanford_output['sentences']:\n",
        "        tokens = []\n",
        "        for token in sentence['tokens']:\n",
        "            token['start'] = int(token['characterOffsetBegin'])\n",
        "            token['end'] = int(token['characterOffsetEnd'])\n",
        "            token['text'] = text[token['start']:token['end']]\n",
        "            if token['text'].strip() in ['\\n', '\\t', ' ', '']:\n",
        "                continue\n",
        "            # Make sure that the token text does not contain any space\n",
        "            if len(token['text'].split(' ')) != 1:\n",
        "                print(\"WARNING: the text of the token contains space character, replaced with hyphen\\n\\t{0}\\n\\t{1}\".format(token['text'], \n",
        "                                                                                                                           token['text'].replace(' ', '-')))\n",
        "                token['text'] = token['text'].replace(' ', '-')\n",
        "            tokens.append(token)\n",
        "        sentences.append(tokens)\n",
        "    return sentences\n",
        "\n",
        "def get_entities_from_brat(text_filepath, annotation_filepath, verbose=False):\n",
        "    # load text\n",
        "    with codecs.open(text_filepath, 'r', 'UTF-8') as f:\n",
        "        text =f.read()\n",
        "    if verbose: print(\"\\ntext:\\n{0}\\n\".format(text))\n",
        "\n",
        "    # parse annotation file\n",
        "    entities = []\n",
        "    with codecs.open(annotation_filepath, 'r', 'UTF-8') as f:\n",
        "        for line in f.read().splitlines():\n",
        "            anno = line.split()\n",
        "            id_anno = anno[0]\n",
        "            # parse entity\n",
        "            if id_anno[0] == 'T':\n",
        "                entity = {}\n",
        "                entity['id'] = id_anno\n",
        "                entity['type'] = anno[1]\n",
        "                entity['start'] = int(anno[2])\n",
        "                entity['end'] = int(anno[3])\n",
        "                entity['text'] = ' '.join(anno[4:])\n",
        "                if verbose:\n",
        "                    print(\"entity: {0}\".format(entity))\n",
        "                # Check compatibility between brat text and anootation\n",
        "                if utils_nlp.replace_unicode_whitespaces_with_ascii_whitespace(text[entity['start']:entity['end']]) != \\\n",
        "                    utils_nlp.replace_unicode_whitespaces_with_ascii_whitespace(entity['text']):\n",
        "                    print(\"Warning: brat text and annotation do not match.\")\n",
        "                    print(\"\\ttext: {0}\".format(text[entity['start']:entity['end']]))\n",
        "                    print(\"\\tanno: {0}\".format(entity['text']))\n",
        "                # add to entitys data\n",
        "                entities.append(entity)\n",
        "    if verbose: print(\"\\n\\n\")\n",
        "    \n",
        "    return text, entities\n",
        "\n",
        "def check_brat_annotation_and_text_compatibility(brat_folder):\n",
        "    '''\n",
        "    Check if brat annotation and text files are compatible.\n",
        "    '''\n",
        "    dataset_type =  os.path.basename(brat_folder)\n",
        "    print(\"Checking the validity of BRAT-formatted {0} set... \".format(dataset_type), end='')\n",
        "    text_filepaths = sorted(glob.glob(os.path.join(brat_folder, '*.txt')))\n",
        "    for text_filepath in text_filepaths:\n",
        "        base_filename = os.path.splitext(os.path.basename(text_filepath))[0]\n",
        "        annotation_filepath = os.path.join(os.path.dirname(text_filepath), base_filename + '.ann')\n",
        "        # check if annotation file exists\n",
        "        if not os.path.exists(annotation_filepath):\n",
        "            raise IOError(\"Annotation file does not exist: {0}\".format(annotation_filepath))\n",
        "        text, entities = get_entities_from_brat(text_filepath, annotation_filepath)\n",
        "    print(\"Done.\")\n",
        "\n",
        "def brat_to_conll(input_folder, output_filepath, tokenizer, language):\n",
        "    '''\n",
        "    Assumes '.txt' and '.ann' files are in the input_folder.\n",
        "    Checks for the compatibility between .txt and .ann at the same time.\n",
        "    '''\n",
        "    if tokenizer == 'spacy':\n",
        "        spacy_nlp = spacy.load(language)\n",
        "    elif tokenizer == 'stanford':\n",
        "        core_nlp = StanfordCoreNLP('http://localhost:{0}'.format(9000))\n",
        "    else:\n",
        "        raise ValueError(\"tokenizer should be either 'spacy' or 'stanford'.\")\n",
        "    verbose = False\n",
        "    dataset_type =  os.path.basename(input_folder)\n",
        "    print(\"Formatting {0} set from BRAT to CONLL... \".format(dataset_type), end='')\n",
        "    text_filepaths = sorted(glob.glob(os.path.join(input_folder, '*.txt')))\n",
        "    output_file = codecs.open(output_filepath, 'w', 'utf-8')\n",
        "    for text_filepath in text_filepaths:\n",
        "        base_filename = os.path.splitext(os.path.basename(text_filepath))[0]\n",
        "        annotation_filepath = os.path.join(os.path.dirname(text_filepath), base_filename + '.ann')\n",
        "        # create annotation file if it does not exist\n",
        "        if not os.path.exists(annotation_filepath):\n",
        "            codecs.open(annotation_filepath, 'w', 'UTF-8').close()\n",
        "\n",
        "        text, entities = get_entities_from_brat(text_filepath, annotation_filepath)\n",
        "        entities = sorted(entities, key=lambda entity:entity[\"start\"])\n",
        "        \n",
        "        if tokenizer == 'spacy':\n",
        "            sentences = get_sentences_and_tokens_from_spacy(text, spacy_nlp)\n",
        "        elif tokenizer == 'stanford':\n",
        "            sentences = get_sentences_and_tokens_from_stanford(text, core_nlp)\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            inside = False\n",
        "            previous_token_label = 'O'\n",
        "            for token in sentence:\n",
        "                token['label'] = 'O'\n",
        "                for entity in entities:\n",
        "                    if entity['start'] <= token['start'] < entity['end'] or \\\n",
        "                       entity['start'] < token['end'] <= entity['end'] or \\\n",
        "                       token['start'] < entity['start'] < entity['end'] < token['end']:\n",
        "\n",
        "                        token['label'] = entity['type'].replace('-', '_') # Because the ANN doesn't support tag with '-' in it\n",
        "\n",
        "                        break\n",
        "                    elif token['end'] < entity['start']:\n",
        "                        break\n",
        "                        \n",
        "                if len(entities) == 0:\n",
        "                    entity={'end':0}\n",
        "                if token['label'] == 'O':\n",
        "                    gold_label = 'O'\n",
        "                    inside = False\n",
        "                elif inside and token['label'] == previous_token_label:\n",
        "                    gold_label = 'I-{0}'.format(token['label'])\n",
        "                else:\n",
        "                    inside = True\n",
        "                    gold_label = 'B-{0}'.format(token['label'])\n",
        "                if token['end'] == entity['end']:\n",
        "                    inside = False\n",
        "                previous_token_label = token['label']\n",
        "                if verbose: print('{0} {1} {2} {3} {4}\\n'.format(token['text'], base_filename, token['start'], token['end'], gold_label))\n",
        "                output_file.write('{0} {1} {2} {3} {4}\\n'.format(token['text'], base_filename, token['start'], token['end'], gold_label))\n",
        "            if verbose: print('\\n')\n",
        "            output_file.write('\\n')\n",
        "\n",
        "    output_file.close()\n",
        "    print('Done.')\n",
        "    if tokenizer == 'spacy':\n",
        "        del spacy_nlp\n",
        "    elif tokenizer == 'stanford':\n",
        "        del core_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3pSXZURUqmR"
      },
      "source": [
        "Las anotaciones se convierten en un único archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCZlF8g5PDlX",
        "outputId": "7da9649b-4a38-45f7-d7cf-9d2d0d7ae006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting task1 set from BRAT to CONLL... WARNING: the text of the token contains space character, replaced with hyphen\n",
            "\tEE. UU.\n",
            "\tEE.-UU.\n",
            "WARNING: the text of the token contains space character, replaced with hyphen\n",
            "\tEE. UU.\n",
            "\tEE.-UU.\n",
            "WARNING: the text of the token contains space character, replaced with hyphen\n",
            "\tEE. UU.\n",
            "\tEE.-UU.\n",
            "Done.\n",
            "Formatting task2 set from BRAT to CONLL... WARNING: the text of the token contains space character, replaced with hyphen\n",
            "\tEE. UU.\n",
            "\tEE.-UU.\n",
            "WARNING: the text of the token contains space character, replaced with hyphen\n",
            "\tEE. UU.\n",
            "\tEE.-UU.\n",
            "WARNING: the text of the token contains space character, replaced with hyphen\n",
            "\tEE. UU.\n",
            "\tEE.-UU.\n",
            "Done.\n",
            "Formatting class set from BRAT to CONLL... Done.\n",
            "Formatting ner set from BRAT to CONLL... Done.\n"
          ]
        }
      ],
      "source": [
        "# Poner el tokenizador en espanol\n",
        "\n",
        "brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task1\", \"task1neuronersm.txt\", \"spacy\", \"es_core_news_sm\")\n",
        "brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task2\", \"task2neuronersm.txt\", \"spacy\", \"es_core_news_sm\")\n",
        "brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/class\", \"classneuronersm.txt\", \"spacy\", \"es_core_news_sm\")\n",
        "brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/ner\", \"nerneuronersm.txt\", \"spacy\", \"es_core_news_sm\")\n",
        "\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task1\", \"task1neuroner.txt\", \"spacy\", \"es_core_news_md\")\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task2\", \"task2neuroner.txt\", \"spacy\", \"es_core_news_md\")\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/class\", \"classneuroner.txt\", \"spacy\", \"es_core_news_md\")\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/ner\", \"nerneuroner.txt\", \"spacy\", \"es_core_news_md\")\n",
        "\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task1\", \"task1neuronersm.txt\", \"spacy\", \"es_core_news_lg\")\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Train/task2\", \"task2neuronersm.txt\", \"spacy\", \"es_core_news_lg\")\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/class\", \"classneuronersm.txt\", \"spacy\", \"es_core_news_lg\")\n",
        "#brat_to_conll(\"/content/drive/MyDrive/Colab Notebooks/Corpus/Original/Test/ner\", \"nerneuronersm.txt\", \"spacy\", \"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6QS1iIjwNDi"
      },
      "source": [
        "Para juntar todos los archivos .ann en uno solo:\n",
        "\n",
        "En la terminal, en MacOS o UNIX nos vamos a la carpeta donde estén los archivos .ann (ejemplo: task1) y lanzamos el siguiente comando\n",
        "\n",
        "https://unix.stackexchange.com/questions/387377/merge-file-text-with-file-name\n",
        "\n",
        "`awk '{print $0,FILENAME}' *.ann >> ~/Desktop/nombredelarchivo.txt`\n",
        "\n",
        "Para instalar awk en MacOS podemos acudir [aquí](https://stackoverflow.com/questions/24332942/why-awk-script-does-not-work-on-mac-os-but-works-on-linux)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}